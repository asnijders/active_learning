#!/bin/bash

#SBATCH --partition=gpu_titanrtx_short
#SBATCH --gres=gpu:1
#SBATCH --job-name=bert_multi
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=4
#SBATCH --time=00:10:00
#SBATCH --mem=32000M
#SBATCH --output=slurm/multi_AL/bert/output_%A.out

module purge
module load 2021
module load Anaconda3/2021.05

cd $HOME/active_learning

# copy all data from home folder to SCRATCH
cp -r $HOME/active_learning/resources/. "$TMPDIR"

# create folder on scratch disk to store output
mkdir "$TMPDIR"/ard_output_dir_checkpoints
mkdir "$TMPDIR"/ard_output_dir_results

# Your job starts in the directory where you call sbatch
cd $HOME/active_learning
# Activate your environment
source activate active

# Run your code
srun python main.py \
            --input_dir "$TMPDIR/ard_data" \
            --output_dir "$TMPDIR/ard_output_dir_results" \
            --checkpoint_dir "$TMPDIR/ard_output_dir_checkpoints" \
            --model_id="bert-base-uncased" \
            --datasets="MNLI,ANLI_R1,ANLI_R2,ANLI_R3" \
            --batch_size=32 \
            --acquisition_fn="random" \
            --labelling_batch_size=7854 \
            --seed_size=7854 \
            --downsample_rate=1.00 \
            --log_every=2 \
            --refresh_rate=25 \
            --seed=43 \
            --al_iterations=10 \
            --num_workers=4 \
            --precision=16 \
            --dropout=0.3 \
            --lr=2e-5 \
            --max_epochs=5 \
            --monitor="val_loss" \
            --uid="bert_mnli_full_anli" \

cp -r "$TMPDIR"/ard_output_dir_results $HOME/active_learning/results