{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-dakota",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-instruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-wyoming",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/ardsnijders/Documents/GitHub/Lisa/active_learning/resources/ard_data/multinli_1.0/multinli_1.0_train.jsonl'\n",
    "# df = pd.read_json(path, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "labeled-chair",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ardsnijders/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json\n",
      "/Users/ardsnijders/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-contributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "arranged-association",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3200it [00:06, 527.16it/s]\n",
      "52it [00:00, 516.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6400it [00:12, 527.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def replace_labels(label):\n",
    "    label_conversions = {'e': 'entailment',\n",
    "                         'c': 'contradiction',\n",
    "                         'n': 'neutral'}\n",
    "\n",
    "    return label_conversions[label]\n",
    "\n",
    "anli_dfs = []\n",
    "for split in ['dev', 'test']:\n",
    "\n",
    "    for data_round in ['R1', 'R2', 'R3']:\n",
    "\n",
    "        data_path = '/Users/ardsnijders/Documents/GitHub/active_learning/resources/data/anli_v1.0/{}/{}.jsonl'.format(data_round, split)\n",
    "        anli_dataset = pd.read_json(data_path, lines=True)\n",
    "        anli_dataset = anli_dataset[['context', 'hypothesis', 'label', 'uid']]  # get rid of unnecessary columns\n",
    "        anli_dataset['label'] = anli_dataset['label'].apply(replace_labels)  # ensures consistently named labels\n",
    "        anli_dfs.append(anli_dataset)\n",
    "        \n",
    "    dataset = pd.concat(anli_dfs, axis=0)\n",
    "    dataset.columns = ['Premise', 'Hypothesis', 'Label', 'ID']\n",
    "    \n",
    "    lengths = []\n",
    "    for i, row in tqdm(dataset.iterrows()):\n",
    "        premise = row['Premise']\n",
    "        hypothesis = row['Hypothesis']\n",
    "\n",
    "        # tokenize sentence and convert to sequence of ids\n",
    "        tokenized_input_seq_pair = tokenizer.encode_plus(text=premise,\n",
    "                                                              text_pair=hypothesis,\n",
    "                                                              add_special_tokens=True,\n",
    "                                                              max_length=2000,\n",
    "                                                              padding='do_not_pad',\n",
    "                                                              return_attention_mask=True,\n",
    "                                                              return_token_type_ids=True,\n",
    "                                                              return_tensors='pt',\n",
    "                                                              truncation=False)\n",
    "\n",
    "        ex_length = tokenized_input_seq_pair.input_ids.squeeze().size()[0]\n",
    "        lengths.append(ex_length)\n",
    "    \n",
    "    lengths = np.array(lengths)\n",
    "    print(lengths.max())\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-organic",
   "metadata": {},
   "outputs": [],
   "source": [
    "345"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
