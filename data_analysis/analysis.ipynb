{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "thick-dinner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>context</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "      <th>model_label</th>\n",
       "      <th>emturk</th>\n",
       "      <th>genre</th>\n",
       "      <th>reason</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81915</th>\n",
       "      <td>0e98e1de-3c7f-4906-bed1-76d507555068</td>\n",
       "      <td>4 of 9 Courtesy of Heidi Klum Diet Coke Campai...</td>\n",
       "      <td>Klum wore multiple looks for the campaign.</td>\n",
       "      <td>e</td>\n",
       "      <td>e</td>\n",
       "      <td>False</td>\n",
       "      <td>news</td>\n",
       "      <td></td>\n",
       "      <td>r3_train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        uid  \\\n",
       "81915  0e98e1de-3c7f-4906-bed1-76d507555068   \n",
       "\n",
       "                                                 context  \\\n",
       "81915  4 of 9 Courtesy of Heidi Klum Diet Coke Campai...   \n",
       "\n",
       "                                       hypothesis label model_label  emturk  \\\n",
       "81915  Klum wore multiple looks for the campaign.     e           e   False   \n",
       "\n",
       "      genre reason       tag  \n",
       "81915  news         r3_train  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ANLI_R3_train = pd.read_json('/Users/ardsnijders/Documents/GitHub/Lisa/active_learning/resources/ard_data/anli_v1.0/R3/train.jsonl', lines=True)\n",
    "\n",
    "ANLI_R3_train[ANLI_R3_train['uid']=='0e98e1de-3c7f-4906-bed1-76d507555068']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "noble-parallel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round</th>\n",
       "      <th>ID</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0e98e1de-3c7f-4906-bed1-76d507555068</td>\n",
       "      <td>ANLI</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>114990</td>\n",
       "      <td>WANLI</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>b910d7cd-76bb-40b5-81bb-b9e4a9f53e81</td>\n",
       "      <td>ANLI</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>fc5aa613-243c-4d22-afe0-3ad6aaad3540</td>\n",
       "      <td>ANLI</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>289606</td>\n",
       "      <td>WANLI</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4245</th>\n",
       "      <td>15</td>\n",
       "      <td>296738</td>\n",
       "      <td>WANLI</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4246</th>\n",
       "      <td>15</td>\n",
       "      <td>30618</td>\n",
       "      <td>WANLI</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4247</th>\n",
       "      <td>15</td>\n",
       "      <td>4688989316.jpg#3r1e</td>\n",
       "      <td>SNLI</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4248</th>\n",
       "      <td>15</td>\n",
       "      <td>188121</td>\n",
       "      <td>WANLI</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4249</th>\n",
       "      <td>15</td>\n",
       "      <td>320623</td>\n",
       "      <td>WANLI</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4250 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      round                                    ID Dataset          Label\n",
       "0         0  0e98e1de-3c7f-4906-bed1-76d507555068    ANLI     entailment\n",
       "1         0                                114990   WANLI  contradiction\n",
       "2         0  b910d7cd-76bb-40b5-81bb-b9e4a9f53e81    ANLI     entailment\n",
       "3         0  fc5aa613-243c-4d22-afe0-3ad6aaad3540    ANLI     entailment\n",
       "4         0                                289606   WANLI     entailment\n",
       "...     ...                                   ...     ...            ...\n",
       "4245     15                                296738   WANLI     entailment\n",
       "4246     15                                 30618   WANLI  contradiction\n",
       "4247     15                   4688989316.jpg#3r1e    SNLI     entailment\n",
       "4248     15                                188121   WANLI  contradiction\n",
       "4249     15                                320623   WANLI  contradiction\n",
       "\n",
       "[4250 rows x 4 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/Users/ardsnijders/Documents/GitHub/Lisa/active_learning/results/ard_output_dir_results/acquisition_IDs/array/OOD_MNLI_active_learning_array_donderdag/roberta-large'\n",
    "\n",
    "strategies = ['bald', 'dal', 'max-entropy', 'mc-max-entropy', 'random']\n",
    "\n",
    "df = pd.read_csv('/Users/ardsnijders/Documents/GitHub/Lisa/active_learning/results/ard_output_dir_results/acquisition_IDs/array/OOD_MNLI_active_learning_array_donderdag/roberta-large/bald/42/acquisition_ids.csv')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bearing-dakota",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "announced-instruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-wyoming",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/ardsnijders/Documents/GitHub/Lisa/active_learning/resources/ard_data/multinli_1.0/multinli_1.0_train.jsonl'\n",
    "# df = pd.read_json(path, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "labeled-chair",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ardsnijders/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json\n",
      "/Users/ardsnijders/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-contributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "arranged-association",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3200it [00:06, 527.16it/s]\n",
      "52it [00:00, 516.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6400it [00:12, 527.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def replace_labels(label):\n",
    "    label_conversions = {'e': 'entailment',\n",
    "                         'c': 'contradiction',\n",
    "                         'n': 'neutral'}\n",
    "\n",
    "    return label_conversions[label]\n",
    "\n",
    "anli_dfs = []\n",
    "for split in ['dev', 'test']:\n",
    "\n",
    "    for data_round in ['R1', 'R2', 'R3']:\n",
    "\n",
    "        data_path = '/Users/ardsnijders/Documents/GitHub/active_learning/resources/data/anli_v1.0/{}/{}.jsonl'.format(data_round, split)\n",
    "        anli_dataset = pd.read_json(data_path, lines=True)\n",
    "        anli_dataset = anli_dataset[['context', 'hypothesis', 'label', 'uid']]  # get rid of unnecessary columns\n",
    "        anli_dataset['label'] = anli_dataset['label'].apply(replace_labels)  # ensures consistently named labels\n",
    "        anli_dfs.append(anli_dataset)\n",
    "        \n",
    "    dataset = pd.concat(anli_dfs, axis=0)\n",
    "    dataset.columns = ['Premise', 'Hypothesis', 'Label', 'ID']\n",
    "    \n",
    "    lengths = []\n",
    "    for i, row in tqdm(dataset.iterrows()):\n",
    "        premise = row['Premise']\n",
    "        hypothesis = row['Hypothesis']\n",
    "\n",
    "        # tokenize sentence and convert to sequence of ids\n",
    "        tokenized_input_seq_pair = tokenizer.encode_plus(text=premise,\n",
    "                                                              text_pair=hypothesis,\n",
    "                                                              add_special_tokens=True,\n",
    "                                                              max_length=2000,\n",
    "                                                              padding='do_not_pad',\n",
    "                                                              return_attention_mask=True,\n",
    "                                                              return_token_type_ids=True,\n",
    "                                                              return_tensors='pt',\n",
    "                                                              truncation=False)\n",
    "\n",
    "        ex_length = tokenized_input_seq_pair.input_ids.squeeze().size()[0]\n",
    "        lengths.append(ex_length)\n",
    "    \n",
    "    lengths = np.array(lengths)\n",
    "    print(lengths.max())\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-organic",
   "metadata": {},
   "outputs": [],
   "source": [
    "345"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
